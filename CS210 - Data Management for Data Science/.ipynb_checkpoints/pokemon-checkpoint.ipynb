{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47f60ace-bbf6-464c-adbd-ca5560c0dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.1\n",
    "import csv\n",
    "\n",
    "def task_1_1(file) :\n",
    "    with open(file) as pokemonFile :\n",
    "        reader = csv.reader(pokemonFile)\n",
    "        next(reader)  # skip header line\n",
    "        fire_count = 0\n",
    "        fire_40_count = 0\n",
    "        for line in reader :\n",
    "            if line[4] == 'fire' :\n",
    "                fire_count = fire_count + 1\n",
    "                if float(line[2]) >= 40 :\n",
    "                    fire_40_count = fire_40_count + 1\n",
    "    percentage = round((fire_40_count / fire_count) * 100)\n",
    "    result = 'Percentage of fire type Pokemons at or above level 40 = ' + str(percentage)\n",
    "    resultFile = open('pokemon1.txt', 'w')\n",
    "    resultFile.write(result)\n",
    "    resultFile.close()\n",
    "\n",
    "task_1_1('pokemonTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ded65360-b6de-4f14-91fd-0df902c2d33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fighting normal\n",
      "flying fighting\n",
      "fire grass\n",
      "water fire\n",
      "poison fairy\n",
      "grass rock\n",
      "ground electric\n",
      "rock fire\n"
     ]
    }
   ],
   "source": [
    "# Tasks 1.2 and 1.3\n",
    "import csv\n",
    "\n",
    "def tasks_1_2_and_1_3(file) :\n",
    "#   Task 1.2 begins here  \n",
    "    with open(file) as pokemonFile :\n",
    "        reader = csv.reader(pokemonFile)\n",
    "        next(reader)  # skip header line\n",
    "        pokemon_dict = {}\n",
    "        for line in reader :\n",
    "            if line[4] == 'NaN' :\n",
    "                continue\n",
    "            if not (line[5] in pokemon_dict.keys()) :\n",
    "                pokemon_dict[line[5]] = [line[4]]\n",
    "            else :\n",
    "                pokemon_dict[line[5]].append(line[4])\n",
    "    for k,v in pokemon_dict.items() :\n",
    "        occurrences = {}\n",
    "        for _type in v :\n",
    "            if _type in occurrences :\n",
    "                occurrences[_type] += 1\n",
    "            else :\n",
    "                occurrences[_type] = 1\n",
    "        max_val = max(list(occurrences.values()))\n",
    "        indices = [index for index, val in enumerate(list(occurrences.values())) if val == max_val]\n",
    "     #  choose type by sorting tied items alphabetically and choosing whichever occurs first\n",
    "        if len(indices) != 1 :\n",
    "            list_to_sort = []\n",
    "            for index in indices :\n",
    "                list_to_sort.append(list(occurrences)[index])\n",
    "            list_to_sort.sort()\n",
    "            pokemon_dict.update({k:list_to_sort[0]})\n",
    "     #  choose type by finding most frequently appearing type for that specific weakness\n",
    "        else :\n",
    "            pokemon_dict.update({k:list(occurrences)[indices[0]]})\n",
    "    for k,v in pokemon_dict.items() :\n",
    "        print(k,v)\n",
    "\n",
    "#   Task 1.3 begins here  \n",
    "    atk_above_40_total = 0\n",
    "    atk_above_40_count = 0\n",
    "    atk_below_40_total = 0\n",
    "    atk_below_40_count = 0\n",
    "    def_above_40_total = 0\n",
    "    def_above_40_count = 0\n",
    "    def_below_40_total = 0\n",
    "    def_below_40_count = 0\n",
    "    hp_above_40_total = 0\n",
    "    hp_above_40_count = 0\n",
    "    hp_below_40_total = 0\n",
    "    hp_below_40_count = 0\n",
    "    with open(file) as pokemonFile :\n",
    "        reader = csv.reader(pokemonFile)\n",
    "        next(reader)  # skip header line\n",
    "        for line in reader :\n",
    "            if line[6] == 'NaN' :\n",
    "                pass\n",
    "            else :\n",
    "                if float(line[2]) > 40 :\n",
    "                    atk_above_40_total += float(line[6])\n",
    "                    atk_above_40_count += 1\n",
    "                else :\n",
    "                    atk_below_40_total += float(line[6])\n",
    "                    atk_below_40_count += 1\n",
    "            if line[7] == 'NaN' :\n",
    "                pass\n",
    "            else :\n",
    "                if float(line[2]) > 40 :\n",
    "                    def_above_40_total += float(line[7])\n",
    "                    def_above_40_count += 1\n",
    "                else :\n",
    "                    def_below_40_total += float(line[7])\n",
    "                    def_below_40_count += 1\n",
    "            if line[8] == 'NaN' :\n",
    "                pass\n",
    "            else :\n",
    "                if float(line[2]) > 40 :\n",
    "                    hp_above_40_total += float(line[8])\n",
    "                    hp_above_40_count += 1\n",
    "                else :\n",
    "                    hp_below_40_total += float(line[8])\n",
    "                    hp_below_40_count += 1\n",
    "    atk_above_40_average = round(atk_above_40_total / atk_above_40_count, 1)\n",
    "    atk_below_40_average = round(atk_below_40_total / atk_above_40_count, 1)\n",
    "    def_above_40_average = round(def_above_40_total / def_above_40_count, 1)\n",
    "    def_below_40_average = round(def_below_40_total / def_above_40_count, 1)\n",
    "    hp_above_40_average = round(hp_above_40_total / hp_above_40_count, 1)\n",
    "    hp_below_40_average = round(hp_below_40_total / hp_above_40_count, 1)\n",
    "\n",
    "    with open(file) as pokemonFile :\n",
    "        reader = csv.reader(pokemonFile)\n",
    "        resultFile = open('pokemonResult.csv', 'w', newline='')\n",
    "        writer = csv.writer(resultFile)\n",
    "        for line in reader :\n",
    "            if line[4] == 'NaN' :\n",
    "                line[4] = pokemon_dict.get(line[5])\n",
    "            if line[6] == 'NaN' :\n",
    "                if float(line[2]) > 40 :\n",
    "                    line[6] = atk_above_40_average\n",
    "                else :\n",
    "                    line[6] = atk_below_40_average\n",
    "            if line[7] == 'NaN' :\n",
    "                if float(line[2]) > 40 :\n",
    "                    line[7] = def_above_40_average\n",
    "                else :\n",
    "                    line[7] = def_below_40_average\n",
    "            if line[8] == 'NaN' :\n",
    "                if float(line[2]) > 40 :\n",
    "                    line[8] = hp_above_40_average\n",
    "                else :\n",
    "                    line[8] = hp_below_40_average\n",
    "            writer.writerow(line)\n",
    "        resultFile.close()\n",
    "                \n",
    "tasks_1_2_and_1_3('pokemonTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "431d7b10-73d7-4e3a-9051-52387acb5e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pokemon type to personality mapping:\n",
      "\n",
      "   normal: mild, relaxed, quiet, calm, naughty, lax, brave, serious, jolly\n",
      "   fire: docile, modest, gentle, bold, brave, rash, naughty, impish, timid, bashful, hardy, jolly, lax\n",
      "   fighting: lonely, serious, hasty, adamant, careful\n",
      "   grass: sassy, naive, lonely, rash, bashful, quiet, gentle, docile, bold, impish, calm, adamant, mild\n",
      "   fairy: impish, sassy, naughty\n",
      "   rock: naughty, mild, bashful, docile, impish\n",
      "   ground: gentle, quiet, hardy, bashful\n",
      "   bug: careful\n",
      "   water: hardy\n",
      "   flying: impish\n",
      "   electric: hardy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 1.4\n",
    "import csv\n",
    "\n",
    "def task_1_4(file):\n",
    "    with open(file) as pokemonFile :\n",
    "        reader = csv.reader(pokemonFile)\n",
    "        next(reader)  # skip header line\n",
    "        type_to_personality_dict = {}\n",
    "        for line in reader :\n",
    "            if not (line[4] in type_to_personality_dict.keys()) :\n",
    "                type_to_personality_dict[line[4]] = [line[3]]\n",
    "            else :\n",
    "                if line[3] in type_to_personality_dict.get(line[4]) :\n",
    "                    continue\n",
    "                else :\n",
    "                    type_to_personality_dict[line[4]].append(line[3])\n",
    "    result = 'Pokemon type to personality mapping:\\n\\n'\n",
    "    for k,v in type_to_personality_dict.items() :\n",
    "        result += '   ' + k + ': '\n",
    "        for personality in v :\n",
    "            if personality == v[-1] :\n",
    "                result += personality + '\\n'\n",
    "            else :\n",
    "                result += personality + ', '\n",
    "    resultFile = open('pokemon4.txt', 'w')\n",
    "    resultFile.write(result)\n",
    "    resultFile.close()\n",
    "    \n",
    "task_1_4('pokemonResult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba88bfee-ff5c-47b8-b5d3-73731679da64",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fbbb34f0f676>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# return newDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mtypesToPersoanlities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pokemonResult.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-fbbb34f0f676>\u001b[0m in \u001b[0;36mtypesToPersoanlities\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mdictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# return lines[1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Task 1.4\n",
    "\n",
    "def typesToPersoanlities(f):\n",
    "    lines = []\n",
    "    dictionary = {}\n",
    "\n",
    "    with open (f, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    file.close()\n",
    "\n",
    "    lines = lines[1:] # gets rid of first line\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        lines[i] = lines[i][:-1].split(\",\")\n",
    "\n",
    "    for i in lines:\n",
    "        dictionary[i[3]] = [i[4]]\n",
    "\n",
    "    # return lines[1]\n",
    "    return dictionary\n",
    "    # newDict = {}\n",
    "\n",
    "    # for i in dictionary.keys():\n",
    "    # key = dictionary[i]\n",
    "\n",
    "    # if not key in newDict.keys():\n",
    "    # newDict[key] = []\n",
    "\n",
    "    # newDict[key].append(i);\n",
    "\n",
    "    # return newDict\n",
    "\n",
    "typesToPersoanlities(\"pokemonResult.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dbc4869c-25ef-4ae9-bd13-af39d9aa6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2; All 5 sub-tasks are implemented in the one single function below\n",
    "\n",
    "import csv\n",
    "\n",
    "def problem_2(file) :\n",
    "    with open(file) as covidFile :\n",
    "        reader = csv.reader(covidFile)\n",
    "        next(reader)\n",
    "        lat_dict = {}\n",
    "        for line in reader :\n",
    "            if line[6] == 'NaN' :\n",
    "                continue\n",
    "            if not (line[4] in lat_dict.keys()) :\n",
    "                lat_dict[line[4]] = [line[6]]\n",
    "            else :\n",
    "                lat_dict[line[4]].append(line[6])    \n",
    "        for k,v in lat_dict.items() : \n",
    "            sum = 0\n",
    "            for string in v :\n",
    "                sum += float(string)\n",
    "                lat_avg = round((sum / len(v)), 2)\n",
    "                lat_dict.update({k:lat_avg})\n",
    "            \n",
    "    with open(file) as covidFile :\n",
    "        reader = csv.reader(covidFile)\n",
    "        next(reader)\n",
    "        long_dict = {}\n",
    "        for line in reader :\n",
    "            if line[7] == 'NaN' :\n",
    "                continue\n",
    "            if not (line[4] in long_dict.keys()) :\n",
    "                long_dict[line[4]] = [line[7]]\n",
    "            else :\n",
    "                long_dict[line[4]].append(line[7])    \n",
    "        for k,v in long_dict.items() : \n",
    "            sum = 0\n",
    "            for string in v :\n",
    "                sum += float(string)\n",
    "                long_avg = round((sum / len(v)), 2)\n",
    "                long_dict.update({k:long_avg})\n",
    "\n",
    "    with open(file) as covidFile :\n",
    "        reader = csv.reader(covidFile)\n",
    "        next(reader)\n",
    "        province_to_cities_dict = {}\n",
    "        for line in reader :\n",
    "            if line[3] == 'NaN' :\n",
    "                continue\n",
    "            if not (line[4] in province_to_cities_dict.keys()) :\n",
    "                province_to_cities_dict[line[4]] = [line[3]]\n",
    "            else :\n",
    "                province_to_cities_dict[line[4]].append(line[3])\n",
    "        \n",
    "    for k,v in province_to_cities_dict.items() :\n",
    "        occurrences = {}\n",
    "        for _type in v :\n",
    "            if _type in occurrences :\n",
    "                occurrences[_type] += 1\n",
    "            else :\n",
    "                occurrences[_type] = 1\n",
    "        max_val = max(list(occurrences.values()))\n",
    "        indices = [index for index, val in enumerate(list(occurrences.values())) if val == max_val]\n",
    "     #  choose type by sorting tied items alphabetically and choosing whichever occurs first\n",
    "        if len(indices) != 1 :\n",
    "            list_to_sort = []\n",
    "            for index in indices :\n",
    "                list_to_sort.append(list(occurrences)[index])\n",
    "            list_to_sort.sort()\n",
    "            province_to_cities_dict.update({k:list_to_sort[0]})\n",
    "     #  choose type by finding most frequently appearing type for that specific weakness\n",
    "        else :\n",
    "            province_to_cities_dict.update({k:list(occurrences)[indices[0]]})\n",
    "            \n",
    "    with open(file) as covidFile :\n",
    "        reader = csv.reader(covidFile)\n",
    "        next(reader)\n",
    "        province_to_symptoms_dict = {}\n",
    "        for line in reader :\n",
    "            if line[11] == 'NaN' :\n",
    "                continue\n",
    "            if not (line[4] in province_to_symptoms_dict.keys()) :\n",
    "                _list = line[11].split(';')\n",
    "                for item in _list:\n",
    "                    new_item = item.strip()\n",
    "                    province_to_symptoms_dict[line[4]] = [new_item]\n",
    "            else :\n",
    "                _list = line[11].split(';')\n",
    "                for item in _list:\n",
    "                    new_item = item.strip()\n",
    "                    province_to_symptoms_dict[line[4]].append(new_item)\n",
    "        \n",
    "    for k,v in province_to_symptoms_dict.items() :\n",
    "        occurrences = {}\n",
    "        for _type in v :\n",
    "            if _type in occurrences :\n",
    "                occurrences[_type] += 1\n",
    "            else :\n",
    "                occurrences[_type] = 1\n",
    "        max_val = max(list(occurrences.values()))\n",
    "        indices = [index for index, val in enumerate(list(occurrences.values())) if val == max_val]\n",
    "     #  choose type by sorting tied items alphabetically and choosing whichever occurs first\n",
    "        if len(indices) != 1 :\n",
    "            list_to_sort = []\n",
    "            for index in indices :\n",
    "                list_to_sort.append(list(occurrences)[index])\n",
    "            list_to_sort.sort()\n",
    "            v = list_to_sort[0]\n",
    "            province_to_symptoms_dict.update({k:v})\n",
    "     #  choose type by finding most frequently appearing type for that specific weakness\n",
    "        else :\n",
    "            province_to_symptoms_dict.update({k:list(occurrences)[indices[0]]})\n",
    "    \n",
    "    with open(file) as covidFile :\n",
    "        reader = csv.reader(covidFile)\n",
    "        resultFile = open('covidResult.csv', 'w', newline='')\n",
    "        writer = csv.writer(resultFile)\n",
    "        writer.writerow(next(reader))\n",
    "        for line in reader :\n",
    "            if '-' in line[1] :\n",
    "                range = line[1].split('-')\n",
    "                line[1] = round((int(range[0]) + int(range[1])) / 2)\n",
    "            if line[3] == 'NaN' :\n",
    "                line[3] = province_to_cities_dict.get(line[4])\n",
    "            if line[6] == 'NaN' :\n",
    "                line[6] = lat_dict.get(line[4])\n",
    "            if line[7] == 'NaN' :\n",
    "                line[7] = long_dict.get(line[4])\n",
    "            line[8] = line[8].split('.')[1] + '.' + line[8].split('.')[0] + '.' + line[8].split('.')[2]\n",
    "            line[9] = line[9].split('.')[1] + '.' + line[9].split('.')[0] + '.' + line[9].split('.')[2]\n",
    "            line[10] = line[10].split('.')[1] + '.' + line[10].split('.')[0] + '.' + line[10].split('.')[2]\n",
    "            if line[11] == 'NaN' :\n",
    "                line[11] = province_to_symptoms_dict.get(line[4])\n",
    "            writer.writerow(line)\n",
    "        resultFile.close()\n",
    "    \n",
    "problem_2('covidTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c4e81ade-f640-4882-a854-94a5126e115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.1\n",
    "\n",
    "def task_2_1(f): #replaces range of values with avg value\n",
    "    lines = []\n",
    "\n",
    "    with open (f, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    file.close()\n",
    "   \n",
    "    lines = lines[1:]\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        lines[i] = lines[i][:-1].split(\",\")\n",
    "   \n",
    "    for i in lines:\n",
    "        if len(i[1]) > 2:\n",
    "            rangeStr = i[1].split(\"-\")\n",
    "            lowerAge = int(rangeStr[0])\n",
    "            upperAge = int(rangeStr[1])\n",
    "            average = round((upperAge + lowerAge) / 2)\n",
    "            i[1] = average\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return lines\n",
    "\n",
    "# task_2_1(\"covidTrain.csv\") #uncomment and use this to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2da1bcfc-56e2-445f-9621-d4311e0d88aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3 found avgs, need to replace vals now\n",
    "import csv\n",
    "from decimal import *\n",
    "def latLong(f):\n",
    "    lines = []\n",
    "    dic = {}\n",
    "    with open (f, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    file.close()\n",
    "    lines = lines[1:]\n",
    "   \n",
    "    for i in range(len(lines)):\n",
    "        lines[i] = lines[i][:-1].split(\",\")\n",
    "       \n",
    "    for i in lines:\n",
    "        if i[6] == \"NaN\":\n",
    "            continue\n",
    "        else:\n",
    "            string = i[6]\n",
    "            num = float(string)\n",
    "            dic[num] = i[4] #latitude\n",
    "       \n",
    "    newDic = {}\n",
    "   \n",
    "    for i in dic.keys():\n",
    "        key = dic[i]\n",
    "        if not key in newDic.keys():\n",
    "            newDic[key] = []    \n",
    "        newDic[key].append(i);\n",
    "       \n",
    "    latAvg = {}\n",
    "   \n",
    "    for key in newDic.keys():\n",
    "        avrg = round((sum(newDic[key])/len(newDic[key])),2)\n",
    "        latAvg[key] = avrg\n",
    "       \n",
    "    for i in lines: #tryna replace the values\n",
    "            if i[6] != \"NaN\":\n",
    "                continue\n",
    "            else: #is NaN\n",
    "#                 print(i[6])\n",
    "                pass\n",
    "\n",
    "#     print(latAvg,'\\n')\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "       \n",
    "    for i in lines:\n",
    "        if i[7] == \"NaN\":\n",
    "            continue\n",
    "        else:\n",
    "            string = i[7]\n",
    "            num = float(string)\n",
    "            dic[num] = i[7] #latitude\n",
    "       \n",
    "    newDic2 = {}\n",
    "   \n",
    "    for i in dic.keys():\n",
    "        key = dic[i]\n",
    "        if not key in newDic2.keys():\n",
    "            newDic2[key] = []    \n",
    "        newDic2[key].append(i);\n",
    "       \n",
    "    longAvg = {}\n",
    "   \n",
    "    for key in newDic2.keys():\n",
    "        avrg2 = round((sum(newDic2[key])/len(newDic2[key])),2)\n",
    "        longAvg[key] = avrg2\n",
    "       \n",
    "    for i in lines: #tryna replace the values\n",
    "            if i[6] != \"NaN\":\n",
    "                continue\n",
    "            else: #is NaN\n",
    "#                 print(i[6])\n",
    "                pass\n",
    "#     print(longAvg, '\\n')\n",
    "\n",
    "    with open(f) as file :\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for line in reader :\n",
    "#             print(f'Lat.: {line[6]}, Long.: {line[7]}')\n",
    "            if line[6] == 'NaN' :\n",
    "                line[6] = latAvg.get(line[4])\n",
    "            if line[7] == 'NaN' :\n",
    "                line[7] = longAvg.get(line[4])\n",
    "#             print(f'Lat.: {line[6]}, Long.: {line[7]}\\n')\n",
    "    \n",
    "#     return latAvg # this dictionary has the avgs for latitude based on province\n",
    "\n",
    "latLong(\"covidTrain.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "237aa074-6fe6-4de8-8cf5-65f628b6617d",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\aatsa/nltk_data'\n    - 'C:\\\\Users\\\\aatsa\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\aatsa\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\aatsa\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\aatsa\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\aatsa/nltk_data'\n    - 'C:\\\\Users\\\\aatsa\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\aatsa\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\aatsa\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\aatsa\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-190-f3b7f481ed0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m                     \u001b[0mresultFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m \u001b[0mtask_3_part_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-190-f3b7f481ed0c>\u001b[0m in \u001b[0;36mtask_3_part_1\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                     \u001b[1;31m#3.2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                     \u001b[0mstopWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\b('\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34mr'|'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34mr')\\b\\s*'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopWords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\aatsa/nltk_data'\n    - 'C:\\\\Users\\\\aatsa\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\aatsa\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\aatsa\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\aatsa\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#3.1, 3.2, and 3.3\n",
    "'''\n",
    "STILL GOTTA WORK ON THE RECIEVING INPUT PART\n",
    "\n",
    "The input set of documents must be read from a file named \"tfidf_docs.txt\". \n",
    "This file will list all the documents (one per line) you will need to work with. \n",
    "For instance, if you need to work with the set \"doc1.txt\", \"doc2.txt\", and \"doc2.txt\", \n",
    "the input file \"tfidf_docs.txt\" contents will look like this:\n",
    "\n",
    "     doc1.txt\n",
    "     doc2.txt\n",
    "     doc2.txt\n",
    "\n",
    "'''\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "def task_3_part_1():\n",
    "    with open('tfidf_docs.txt', 'r') as initial_file :\n",
    "        list_of_files = initial_file.readlines()\n",
    "        for individual_file in list_of_files :\n",
    "            individual_file = individual_file.strip('\\n')\n",
    "            with open(individual_file, 'r') as file :\n",
    "                lines = file.readlines()\n",
    "                for line in lines :\n",
    "    \n",
    "                    str = \" \" \n",
    "                    string = str.join(lines)\n",
    "\n",
    "                    #3.1\n",
    "                    string1 = re.sub('[^A-Za-z0-9_]+', ' ', string)\n",
    "                    string1 = string1.strip()\n",
    "                    string1 = string1.replace(\"  \",\" \")\n",
    "                    string1 = string1.replace(\"   \",\" \")\n",
    "                    string1 = re.sub(r'http\\S+', '', string1)\n",
    "                    string1 = string1.lower()\n",
    "                    result = string1\n",
    "\n",
    "                    #3.2\n",
    "                    stopWords = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "                    result = stopWords.sub('',result)\n",
    "\n",
    "                    #3.3\n",
    "                    ps = PorterStemmer() \n",
    "                    words = word_tokenize(result)\n",
    "                    totalResult = \"\"\n",
    "                    for i in words:\n",
    "                        totalResult += ps.stem(i) + \" \"\n",
    "\n",
    "                    result = totalResult\n",
    "                    result = result.replace(\"  \",\" \")\n",
    "                    result = result.replace(\"   \",\" \")\n",
    "                    print(result)\n",
    "\n",
    "                    #convert to preproc_ file\n",
    "                    fileName = \"preproc_\"\n",
    "                    fileName += f\n",
    "                    resultFile = open(fileName, 'w')\n",
    "                    resultFile.write(result)\n",
    "                    resultFile.close()\n",
    "\n",
    "task_3_part_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b5679-faa5-4c87-89e9-f319cf91062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.1, 3.2, and 3.3\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "def task_3_part_1():\n",
    "    with open('tfidf_docs.txt', 'r') as initial_file :\n",
    "        list_of_files = initial_file.readlines()\n",
    "        for individual_file in list_of_files :\n",
    "            individual_file = individual_file.strip('\\n')\n",
    "            with open(individual_file, 'r') as file :\n",
    "                lines = file.readlines()\n",
    "                for line in lines :\n",
    "                    str = \" \"\n",
    "                    string = str.join(lines)\n",
    "\n",
    "                    #3.1\n",
    "                    string1 = re.sub('[^A-Za-z0-9_]+', ' ', string)\n",
    "                    string1 = string1.strip()\n",
    "                    string1 = string1.replace(\"  \",\" \")\n",
    "                    string1 = string1.replace(\"   \",\" \")\n",
    "                    string1 = re.sub(r'http\\S+', '', string1)\n",
    "                    string1 = string1.lower()\n",
    "                    result = string1\n",
    "\n",
    "                    #3.2\n",
    "                    stopWords = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "                    result = stopWords.sub('',result)\n",
    "\n",
    "                    #3.3\n",
    "                    ps = PorterStemmer()\n",
    "                    words = word_tokenize(result)\n",
    "                    totalResult = \"\"\n",
    "                    for i in words:\n",
    "                        totalResult += ps.stem(i) + \" \"\n",
    "\n",
    "                    result = totalResult\n",
    "                    result = result.replace(\"  \",\" \")\n",
    "                    result = result.replace(\"   \",\" \")\n",
    "#                     print(result)\n",
    "\n",
    "                    #convert to preproc_ file\n",
    "                    fileName = \"preproc_\"\n",
    "                    fileName += individual_file\n",
    "                    resultFile = open(fileName, 'w')\n",
    "                    resultFile.write(result)\n",
    "                    resultFile.close()\n",
    "\n",
    "task_3_part_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23dbaff-fdf0-4f60-a944-3873bbb0485b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
